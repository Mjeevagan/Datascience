{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab43a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f891c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Selectkbest algorith using Chisquare or RMS formula,it will evaluate the model and provide the best input\n",
    "#Chi square will analyse the data and provide the result as per k value\n",
    "#it provide Rscore value across all the models\n",
    "def selectkbest(indep_X,dep_Y,n):\n",
    "        test = SelectKBest(score_func=chi2, k=n)\n",
    "        fit1= test.fit(indep_X,dep_Y)\n",
    "        # summarize scores       \n",
    "        selectk_features = fit1.transform(indep_X)\n",
    "        return selectk_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3797e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using standard scaler and split the train and test set\n",
    "\n",
    "def split_scalar(indep_X,dep_Y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(indep_X, dep_Y, test_size = 0.25, random_state = 0)\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)    \n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397f9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A confusion matrix is a table that is often used to evaluate the performance of a\n",
    "#classification algorithm on a set of labeled data\n",
    "#It provides a summary of the predictions made by a classification model compared to the actual true values. \n",
    "#The matrix has four entries: true positive (TP), false positive (FP), true negative (TN), and false negative (FN).\n",
    "def cm_prediction(classifier,X_test):\n",
    "     y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        \n",
    "        # Making the Confusion Matrix\n",
    "     from sklearn.metrics import confusion_matrix\n",
    "     cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "     from sklearn.metrics import accuracy_score \n",
    "     from sklearn.metrics import classification_report \n",
    "        #from sklearn.metrics import confusion_matrix\n",
    "        #cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "     Accuracy=accuracy_score(y_test, y_pred )\n",
    "#once above model creates in return  classifier,Accuracy,report,X_test,y_test,cm report will come\n",
    "     report=classification_report(y_test, y_pred)\n",
    "     return  classifier,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c0eb527",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:4\u001b[1;36m\u001b[0m\n\u001b[1;33m    from sklearn.linear_model import LogisticRegression\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# to create the Logistic regression model to find the score\n",
    "def logistic(X_train,y_train,X_test):       \n",
    "         Fitting K-NN to the Training set\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        classifier = LogisticRegression(random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create the SVM Linear model to find the score\n",
    "def svm_linear(X_train,y_train,X_test):\n",
    "                \n",
    "        from sklearn.svm import SVC\n",
    "        classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_NL(X_train,y_train,X_test):\n",
    "                \n",
    "        from sklearn.svm import SVC\n",
    "        classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79647844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Navie(X_train,y_train,X_test):       \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        classifier = GaussianNB()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train,y_train,X_test):\n",
    "           \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea09c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decision(X_train,y_train,X_test):\n",
    "        \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffac1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(X_train,y_train,X_test):\n",
    "        \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for creating table and providing the score\n",
    "def selectk_Classification(acclog,accsvml,accsvmnl,accknn,accnav,accdes,accrf): \n",
    "    \n",
    "    dataframe=pd.DataFrame(index=['ChiSquare'],columns=['Logistic','SVMl','SVMnl','KNN','Navie','Decision','Random'])\n",
    "#enumerate is for creating the index and providing the no in the table\n",
    "    for number,idex in enumerate(dataframe.index):      \n",
    "        dataframe['Logistic'][idex]=acclog[number]       \n",
    "        dataframe['SVMl'][idex]=accsvml[number]\n",
    "        dataframe['SVMnl'][idex]=accsvmnl[number]\n",
    "        dataframe['KNN'][idex]=accknn[number]\n",
    "        dataframe['Navie'][idex]=accnav[number]\n",
    "        dataframe['Decision'][idex]=accdes[number]\n",
    "        dataframe['Random'][idex]=accrf[number]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1=pd.read_csv(\"Creditscore.csv\",index_col=None)\n",
    "df2=dataset1\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ea6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df2)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf70f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_X=df2[['Age','SSN','Annual_Income','Monthly_Inhand_Salary','Num_Bank_Accounts','Outstanding_Debt','Credit_Utilization_Ratio','Credit_History_Age','Total_EMI_per_month','Amount_invested_monthly','Monthly_Balance']]\n",
    "               \n",
    "dep_Y=df2[['Credit_Score_Good','Credit_Score_Poor','Credit_Score_Standard']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb033b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest=selectkbest(indep_X,dep_Y,5)       \n",
    "\n",
    "acclog=[]\n",
    "accsvml=[]\n",
    "accsvmnl=[]\n",
    "accknn=[]\n",
    "accnav=[]\n",
    "accdes=[]\n",
    "accrf=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17940d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=split_scalar(kbest,dep_Y)   \n",
    "    \n",
    "        \n",
    "classifier,Accuracy,report,X_test,y_test,cm=logistic(X_train,y_train,X_test)\n",
    "acclog.append(Accuracy)\n",
    "\n",
    "classifier,Accuracy,report,X_test,y_test,cm=svm_linear(X_train,y_train,X_test)  \n",
    "accsvml.append(Accuracy)\n",
    "    \n",
    "classifier,Accuracy,report,X_test,y_test,cm=svm_NL(X_train,y_train,X_test)  \n",
    "accsvmnl.append(Accuracy)\n",
    "    \n",
    "classifier,Accuracy,report,X_test,y_test,cm=knn(X_train,y_train,X_test)  \n",
    "accknn.append(Accuracy)\n",
    "    \n",
    "classifier,Accuracy,report,X_test,y_test,cm=Navie(X_train,y_train,X_test)  \n",
    "accnav.append(Accuracy)\n",
    "    \n",
    "classifier,Accuracy,report,X_test,y_test,cm=Decision(X_train,y_train,X_test)  \n",
    "accdes.append(Accuracy)\n",
    "    \n",
    "classifier,Accuracy,report,X_test,y_test,cm=random(X_train,y_train,X_test)  \n",
    "accrf.append(Accuracy)\n",
    "    \n",
    "result=selectk_Classification(acclog,accsvml,accsvmnl,accknn,accnav,accdes,accrf)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b13b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
